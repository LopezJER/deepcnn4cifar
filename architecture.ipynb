{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWj7vbXPhUnb",
        "outputId": "4d6ec866-aff7-4fcb-f882-9f644719f85d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "lNDWw16GfVid"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, random_split\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqnhF1zw8i1V",
        "outputId": "712013ec-3d1c-4d39-f388-74f889e875d8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, random_split, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "import torch\n",
        "\n",
        "# Load the CIFAR-10 training data\n",
        "train_data = datasets.CIFAR10(root='./data', train=True, download=True)\n",
        "\n",
        "# Calculate mean and std\n",
        "# as per Github discussions (paulkorir, 2018): https://github.com/facebookarchive/fb.resnet.torch/issues/180#issuecomment-433419706\n",
        "x = np.concatenate([np.asarray(train_data[i][0]) for i in range(len(train_data))])\n",
        "train_mean = np.mean(x, axis=(0, 1)) / 255.0\n",
        "train_std = np.std(x, axis=(0, 1)) / 255.0\n",
        "print(\"Mean:\", train_mean)\n",
        "print(\"Std:\", train_std)\n",
        "\n",
        "# Define transformations\n",
        "# Experiment with/tune augmentations here\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, hue=0.1),\n",
        "    transforms.Normalize(mean=train_mean.tolist(), std=train_std.tolist())\n",
        "])\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.Normalize(mean=train_mean.tolist(), std=train_std.tolist())\n",
        "])\n",
        "\n",
        "class TransformDataset(Dataset):\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.dataset[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Load raw dataset without transforms first\n",
        "train_dataset_raw = datasets.CIFAR10(root='./data', train=True, transform=None, download=True)\n",
        "\n",
        "# Split raw dataset\n",
        "val_split = 0.2\n",
        "train_size = int((1 - val_split) * len(train_dataset_raw))\n",
        "val_size = len(train_dataset_raw) - train_size\n",
        "train_subset_raw, val_subset_raw = random_split(train_dataset_raw, [train_size, val_size])\n",
        "\n",
        "# Create separate datasets with appropriate transforms\n",
        "train_data = TransformDataset(train_subset_raw, train_transform)\n",
        "val_data = TransformDataset(val_subset_raw, val_test_transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=val_test_transform, download=True)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 256\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Print dataset sizes\n",
        "print(f\"Training samples: {len(train_data)}\")\n",
        "print(f\"Validation samples: {len(val_data)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUf_VIzV8_RL",
        "outputId": "96d546bb-2aa9-4487-f9a5-eac40c92ed01"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Mean: [0.49139968 0.48215841 0.44653091]\n",
            "Std: [0.24703223 0.24348513 0.26158784]\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Training samples: 40000\n",
            "Validation samples: 10000\n",
            "Test samples: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iM_oIT0_eYy-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61f4645e-f47e-443e-de9b-74b690e2fe18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Constructing VGG11\n",
            "Epoch [1/1000]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class VGG_Network(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, config='vgg16'):\n",
        "        super(VGG_Network, self).__init__()\n",
        "        conv_blocks = []\n",
        "\n",
        "        if config == 'vgg11':\n",
        "            print(\"Constructing VGG11\")\n",
        "            self.conv2d_block1 = self.conv2d_block(input_size[0], 64, 1)\n",
        "            self.conv2d_block2 = self.conv2d_block(64, 128, 1)\n",
        "            self.conv2d_block3 = self.conv2d_block(128, 256, 2)\n",
        "            self.conv2d_block4 = self.conv2d_block(256, 512, 2)\n",
        "            self.conv2d_block5 = self.conv2d_block(512, 512, 2)\n",
        "\n",
        "        elif config == 'vgg16':\n",
        "            self.conv2d_block1 = self.conv2d_block(input_size[0], 64, 2)\n",
        "            self.conv2d_block2 = self.conv2d_block(64, 128, 2)\n",
        "            self.conv2d_block3 = self.conv2d_block(128, 256, 3)\n",
        "            self.conv2d_block4 = self.conv2d_block(256, 512, 3)\n",
        "            self.conv2d_block5 = self.conv2d_block(512, 512, 3)\n",
        "\n",
        "        self.linear1 = nn.Linear(512 * 7 * 7, 4096)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.linear2 = nn.Linear(4096, 4096)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.linear3 = nn.Linear(4096, num_classes)\n",
        "\n",
        "    def conv2d_block(self, in_channels, out_channels, num_layers):\n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
        "        layers.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        for _ in range(num_layers-1):\n",
        "            layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "            in_channels = out_channels\n",
        "\n",
        "        layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv2d_block1(x)\n",
        "        x = self.conv2d_block2(x)\n",
        "        x = self.conv2d_block3(x)\n",
        "        x = self.conv2d_block4(x)\n",
        "        x = self.conv2d_block5(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.linear3(x)\n",
        "        return nn.functional.log_softmax(x, dim=1) #output\n",
        "\n",
        "# Function to train the model\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, scheduler, patience=5):\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
        "        model.train()\n",
        "\n",
        "        train_running_loss = 0.0\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_running_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        val_running_loss = 0.0\n",
        "        with torch.no_grad():  # Disable gradient calculation\n",
        "            for i, (inputs, labels) in enumerate(val_loader):\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_running_loss += loss.item()\n",
        "\n",
        "        average_train_loss = train_running_loss / len(train_loader)\n",
        "        average_val_loss = val_running_loss / len(val_loader)\n",
        "        scheduler.step(average_val_loss)\n",
        "\n",
        "        # Store losses for plotting\n",
        "        train_losses.append(average_train_loss)\n",
        "        val_losses.append(average_val_loss)\n",
        "\n",
        "        print(f\"Average Train Loss: {average_train_loss:.4f}\")\n",
        "        print(f\"Average Validation Loss: {average_val_loss:.4f}\")\n",
        "\n",
        "        # Check if validation loss improved\n",
        "        if average_val_loss < best_val_loss:\n",
        "            best_val_loss = average_val_loss\n",
        "            epochs_without_improvement = 0\n",
        "            # Save best model\n",
        "            torch.save(model.state_dict(), '/content/drive/MyDrive/best_model.pth')\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement >= patience:\n",
        "                print(f'Early stopping at epoch {epoch+1}')\n",
        "                break\n",
        "\n",
        "    # Save the average losses to a text file\n",
        "    with open('/content/drive/MyDrive/losses.txt', 'w') as f:\n",
        "        f.write(\"Average Train Losses:\\n\")\n",
        "        f.writelines([f\"{loss}\\n\" for loss in train_losses])\n",
        "        f.write(\"\\nAverage Validation Losses:\\n\")\n",
        "        f.writelines([f\"{loss}\\n\" for loss in val_losses])\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "# Example usage\n",
        "def main():\n",
        "    # Model parameters\n",
        "    input_size = (3, 224, 224)\n",
        "    num_classes = 10\n",
        "    learning_rate = 1e-2\n",
        "    num_epochs = 1000 # we use early stopping, so it's not really 100 >.<\n",
        "    weight_decay = 5 * 1e-4\n",
        "    momentum = 0.9\n",
        "\n",
        "    # Initialize the model\n",
        "    model = VGG_Network(input_size, num_classes, config='vgg11').to(device)\n",
        "\n",
        "    # Loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), momentum=momentum, lr=learning_rate, weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.1, verbose=True)\n",
        "\n",
        "    # Train the model and capture losses\n",
        "    train_losses, val_losses = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, scheduler)\n",
        "\n",
        "    # Visualize train and validation loss\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Train and Validation Loss')\n",
        "    plt.savefig('train_val_loss.png')\n",
        "    plt.show()\n",
        "\n",
        "    drive.flush_and_unmount() #important for saved files to reflect quickly on Google Drive if using Colab\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JfdhN5tBA6PX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install graphviz\n"
      ],
      "metadata": {
        "id": "lGK4e3UBA7Mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torchview\n",
        "! pip install git+https://github.com/mert-kurttutan/torchview.git\n"
      ],
      "metadata": {
        "id": "myunrs24BuWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchview import draw_graph\n",
        "input_size = (3, 224, 224)\n",
        "num_classes=10\n",
        "model = VGG_Network(input_size, num_classes, config='vgg11').to(device)\n",
        "# device='meta' -> no memory is consumed for visualization\n",
        "model_graph = draw_graph(model, input_size=(1, 3, 224, 224), device=device)\n",
        "model_graph.visual_graph"
      ],
      "metadata": {
        "id": "vROqqBPwpA9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchview import draw_graph\n",
        "model = VGG_Network(input_size, num_classes, config='vgg16').to(device)\n",
        "# device='meta' -> no memory is consumed for visualization\n",
        "model_graph = draw_graph(model, input_size=(1, 3, 224, 224), device=device)\n",
        "model_graph.visual_graph"
      ],
      "metadata": {
        "id": "hO9Y4nSxCicQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}